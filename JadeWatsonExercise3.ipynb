{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3: Textual Data Analysis\n",
    "##### Jade Watson: 20052115"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.idea', 'CISC_CMPE351_W22_E3.pdf', 'corpus.pkl', 'dictionary.gensim', 'Exercise3.ipynb', 'JadeWatsonExercise3.ipynb', 'model1.gensim', 'model1.gensim.expElogbeta.npy', 'model1.gensim.id2word', 'model1.gensim.state', 'model10.gensim', 'model10.gensim.expElogbeta.npy', 'model10.gensim.id2word', 'model10.gensim.state', 'model5.gensim', 'model5.gensim.expElogbeta.npy', 'model5.gensim.id2word', 'model5.gensim.state', 'Questions.csv', 'Questions.csv.zip', 'run_word2vec.ipynb', 'w2v_model.txt']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "datasetPath = r\"C:/Users/Jade Watson/Documents/CMPE 351/Exercise3//\"\n",
    "print(os.listdir(datasetPath))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "         Id  OwnerUserId          CreationDate  Score  \\\n0  25063739    1980846.0  2014-07-31T16:00:55Z      2   \n1  35306295     996366.0  2016-02-10T03:26:06Z      4   \n2  24127787    3723583.0  2014-06-09T19:46:05Z      0   \n3  40260119    1901071.0  2016-10-26T10:42:00Z      0   \n4  16783551    2426904.0  2013-05-28T03:46:13Z      1   \n5  36937359    4701887.0  2016-04-29T11:37:04Z      2   \n6  44098910    5472628.0  2017-05-21T15:58:18Z      0   \n7  45936144    1819625.0  2017-08-29T10:00:31Z      2   \n8  24425588    2849053.0  2014-06-26T08:16:28Z      1   \n9  36957580    6275599.0  2016-04-30T16:49:21Z      1   \n\n                                               Title  \\\n0  R tcl tk: How do I pass a variable to a button...   \n1  How to stop running shiny app by closing the b...   \n2  Installation error \"no package specified\" - tr...   \n3           Building Sentences from a dataframe in R   \n4         Downloading multiple file as parallel in R   \n5  How to extract the lower triangle of a Distanc...   \n6  What is an appropriate approach to study seque...   \n7                Transition plot with time on x axis   \n8                         Attribute information gain   \n9                             R Sum columns by index   \n\n                                                Body  \n0  <p>How would I pass the value of <code>num</co...  \n1  <p>I have deployed a app in <strong>shinyapps....  \n2  <p><strong>When I tried git-hub install, using...  \n3  <p>Im trying to generate sentences from a data...  \n4  <p>I am trying to download 460,000 files from ...  \n5  <p>I have a distance matrix called <code>mydis...  \n6  <p>I have a data set with customer ID, event_d...  \n7  <p>I have a transition matrix as following:</p...  \n8  <p>I am an R user and I am interested in findi...  \n9  <p><p>I need to find a way to sum columns by t...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>OwnerUserId</th>\n      <th>CreationDate</th>\n      <th>Score</th>\n      <th>Title</th>\n      <th>Body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25063739</td>\n      <td>1980846.0</td>\n      <td>2014-07-31T16:00:55Z</td>\n      <td>2</td>\n      <td>R tcl tk: How do I pass a variable to a button...</td>\n      <td>&lt;p&gt;How would I pass the value of &lt;code&gt;num&lt;/co...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35306295</td>\n      <td>996366.0</td>\n      <td>2016-02-10T03:26:06Z</td>\n      <td>4</td>\n      <td>How to stop running shiny app by closing the b...</td>\n      <td>&lt;p&gt;I have deployed a app in &lt;strong&gt;shinyapps....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>24127787</td>\n      <td>3723583.0</td>\n      <td>2014-06-09T19:46:05Z</td>\n      <td>0</td>\n      <td>Installation error \"no package specified\" - tr...</td>\n      <td>&lt;p&gt;&lt;strong&gt;When I tried git-hub install, using...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>40260119</td>\n      <td>1901071.0</td>\n      <td>2016-10-26T10:42:00Z</td>\n      <td>0</td>\n      <td>Building Sentences from a dataframe in R</td>\n      <td>&lt;p&gt;Im trying to generate sentences from a data...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16783551</td>\n      <td>2426904.0</td>\n      <td>2013-05-28T03:46:13Z</td>\n      <td>1</td>\n      <td>Downloading multiple file as parallel in R</td>\n      <td>&lt;p&gt;I am trying to download 460,000 files from ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>36937359</td>\n      <td>4701887.0</td>\n      <td>2016-04-29T11:37:04Z</td>\n      <td>2</td>\n      <td>How to extract the lower triangle of a Distanc...</td>\n      <td>&lt;p&gt;I have a distance matrix called &lt;code&gt;mydis...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>44098910</td>\n      <td>5472628.0</td>\n      <td>2017-05-21T15:58:18Z</td>\n      <td>0</td>\n      <td>What is an appropriate approach to study seque...</td>\n      <td>&lt;p&gt;I have a data set with customer ID, event_d...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>45936144</td>\n      <td>1819625.0</td>\n      <td>2017-08-29T10:00:31Z</td>\n      <td>2</td>\n      <td>Transition plot with time on x axis</td>\n      <td>&lt;p&gt;I have a transition matrix as following:&lt;/p...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>24425588</td>\n      <td>2849053.0</td>\n      <td>2014-06-26T08:16:28Z</td>\n      <td>1</td>\n      <td>Attribute information gain</td>\n      <td>&lt;p&gt;I am an R user and I am interested in findi...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>36957580</td>\n      <td>6275599.0</td>\n      <td>2016-04-30T16:49:21Z</td>\n      <td>1</td>\n      <td>R Sum columns by index</td>\n      <td>&lt;p&gt;&lt;p&gt;I need to find a way to sum columns by t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Style Dataframe\n",
    "df = pd.read_csv(datasetPath + \"Questions.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cleaning\n",
    "##### Lemmatizing and removing the stopwords and non-alphabetic characters for each line of dialogue."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jade\n",
      "[nltk_data]     Watson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jade Watson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "C:\\Users\\Jade Watson\\AppData\\Local\\Temp\\ipykernel_5684\\2985933192.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Title'] =df['Title'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 18>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     15\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([x\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m word_tokenize(text) \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m punctuations \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x)\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m3\u001B[39m])\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m text\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[1;32m---> 18\u001B[0m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclean_text\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m=\u001B[39m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTitle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m:\u001B[49m\u001B[43mclean_text_initial\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m df\u001B[38;5;241m.\u001B[39mhead()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4357\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4247\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4248\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4249\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4252\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4253\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m FrameOrSeriesUnion:\n\u001B[0;32m   4254\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4255\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4256\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4355\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4356\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1043\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1040\u001B[0m     \u001B[38;5;66;03m# if we are a string, try to dispatch\u001B[39;00m\n\u001B[0;32m   1041\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m-> 1043\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1098\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1092\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m   1093\u001B[0m         \u001B[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001B[39;00m\n\u001B[0;32m   1094\u001B[0m         \u001B[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001B[39;00m\n\u001B[0;32m   1095\u001B[0m         \u001B[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001B[39;00m\n\u001B[0;32m   1096\u001B[0m         \u001B[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001B[39;00m\n\u001B[0;32m   1097\u001B[0m         \u001B[38;5;66;03m# \"Callable[[Any], Any]\"\u001B[39;00m\n\u001B[1;32m-> 1098\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1099\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1100\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1101\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1102\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1105\u001B[0m     \u001B[38;5;66;03m# GH 25959 use pd.array instead of tolist\u001B[39;00m\n\u001B[0;32m   1106\u001B[0m     \u001B[38;5;66;03m# so extension arrays can be used\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(pd_array(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2859\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     15\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([x\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m word_tokenize(text) \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m punctuations \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x)\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m3\u001B[39m])\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m text\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[1;32m---> 18\u001B[0m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclean_text\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39mTitle\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m text:\u001B[43mclean_text_initial\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     19\u001B[0m df\u001B[38;5;241m.\u001B[39mhead()\n",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36mclean_text_initial\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclean_text_initial\u001B[39m(text):\n\u001B[0;32m     14\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([x\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m word_tokenize(text) \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stopwords_list \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x)\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m---> 15\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([x\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m \u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m punctuations \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x)\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m3\u001B[39m])\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m text\u001B[38;5;241m.\u001B[39mstrip()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:130\u001B[0m, in \u001B[0;36mword_tokenize\u001B[1;34m(text, language, preserve_line)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;124;03mReturn a tokenized copy of *text*,\u001B[39;00m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;124;03musing NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;124;03m:type preserve_line: bool\u001B[39;00m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    129\u001B[0m sentences \u001B[38;5;241m=\u001B[39m [text] \u001B[38;5;28;01mif\u001B[39;00m preserve_line \u001B[38;5;28;01melse\u001B[39;00m sent_tokenize(text, language)\n\u001B[1;32m--> 130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m    131\u001B[0m     token \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m _treebank_word_tokenizer\u001B[38;5;241m.\u001B[39mtokenize(sent)\n\u001B[0;32m    132\u001B[0m ]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:131\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;124;03mReturn a tokenized copy of *text*,\u001B[39;00m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;124;03musing NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;124;03m:type preserve_line: bool\u001B[39;00m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    129\u001B[0m sentences \u001B[38;5;241m=\u001B[39m [text] \u001B[38;5;28;01mif\u001B[39;00m preserve_line \u001B[38;5;28;01melse\u001B[39;00m sent_tokenize(text, language)\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m--> 131\u001B[0m     token \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[43m_treebank_word_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    132\u001B[0m ]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\destructive.py:161\u001B[0m, in \u001B[0;36mNLTKWordTokenizer.tokenize\u001B[1;34m(self, text, convert_parentheses, return_str)\u001B[0m\n\u001B[0;32m    158\u001B[0m     text \u001B[38;5;241m=\u001B[39m regexp\u001B[38;5;241m.\u001B[39msub(substitution, text)\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m regexp, substitution \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mPUNCTUATION:\n\u001B[1;32m--> 161\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[43mregexp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubstitution\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;66;03m# Handles parentheses.\u001B[39;00m\n\u001B[0;32m    164\u001B[0m regexp, substitution \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mPARENS_BRACKETS\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\re.py:327\u001B[0m, in \u001B[0;36m_subx\u001B[1;34m(pattern, template)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_subx\u001B[39m(pattern, template):\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;66;03m# internal: Pattern.sub/subn implementation helper\u001B[39;00m\n\u001B[1;32m--> 327\u001B[0m     template \u001B[38;5;241m=\u001B[39m \u001B[43m_compile_repl\u001B[49m(template, pattern)\n\u001B[0;32m    328\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m template[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(template[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    329\u001B[0m         \u001B[38;5;66;03m# literal replacement\u001B[39;00m\n\u001B[0;32m    330\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m template[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['Title'] =df['Title'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "stopwords_list = stopwords.words('english')\n",
    "punctuations = list(set(string.punctuation))\n",
    "\n",
    "def clean_text_initial(text):\n",
    "    text = ' '.join([x.lower() for x in word_tokenize(text) if x.lower() not in stopwords_list and len(x)>1])\n",
    "    text = ' '.join([x.lower() for x in word_tokenize(text) if x.lower() not in punctuations and len(x)>3])\n",
    "    return text.strip()\n",
    "\n",
    "df[\"clean_text\"]=df.Title.apply(lambda text:clean_text_initial(str(text)))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I decided to lemmanize the titles instaed of stemming them directly because it stems the word but ensures that it does not loose its meaning. Lemmatization has a pre-defined dictionary that stores the context of words and checks the word in the dictionary."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "cleaned_text_list=df.clean_text.apply(lambda clean_text:[lemmatizer.lemmatize(tokenized_text) for tokenized_text in word_tokenize(clean_text)])\n",
    "\n",
    "gensim_dict=Dictionary(cleaned_text_list)\n",
    "\n",
    "doc_term_matrix = [gensim_dict.doc2bow(text) for text in cleaned_text_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df.clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bigrams\n",
    "##### Used to detect common phrases from a list of sentances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# make list a series\n",
    "clean_text_series = pd.Series(cleaned_text_list)\n",
    "\n",
    "sent = [row.split() for row in df.clean_text]\n",
    "# create the relevant phrases from the list of sentances\n",
    "phrases = Phrases(sent,min_count=30,progress_per=10000)\n",
    "# use phraser() to cut down memory consumption of phrases() by discarding model state not needed for bigram detection task\n",
    "bigram = Phraser(phrases)\n",
    "# transform the corpus based on the bigrams detected:\n",
    "sentences = bigram[sent]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Most Frequent Words\n",
    "##### Used to check the effectiveness of the lemmatization and addition of bigrams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### i) Word2Vec(): set up parameters of the model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "# count the number of cores in a computer\n",
    "cores = multiprocessing.cpu_count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=25,window=2,size=300,sample=6e-5,alpha=0.03,min_alpha=0.0007,negative=20,workers=cores-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### ii) Build Vocabulary Table: build vocab from a sequence of sentences to initialize the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build the vocabulary table\n",
    "t = time()\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "print('Time to build vocab table: {} mins'.format(round((time()-t)/60,2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### iii) Train the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = time()\n",
    "# where total_examples is the count of sentences\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make model more memory-efficient\n",
    "w2v_model.init_sims(replace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learned_words = list(w2v_model.wv.vocab)\n",
    "print(learned_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save model\n",
    "w2v_model.wv.save_word2vec_format('w2v_model.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploring the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=['plots'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('plot','answer')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Topic Modelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(text) for text in sentences]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', ldamodel.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 10 topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model10.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', ldamodel.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 5 Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "According to our notes, calculating the perplexity is how one chooses the number of topics. To calculate perplexity the inverse log-likelihood of the dictionary above is taken. We wish to have models with lower perplexity because it suggests less uncertainties about the unobserved document. I tested this theory by varying the number of topics. I first tested with 5 topics. It is evident that topic 1 contains words like function, using, loop, and error. Topics 1-4 are printed above as well. Next I tested with 10 topics. Topic 6 contains words like data, table, extract, and remove. The weights in front of each words reflects how important the keyword is to the topic. So the higher the weight the more it contributes to the topic. In the case above, the preferred number of topics is 5. Another approach to finding the optimal number of topics is to build many LDA models with different topic number and pick one that gives the highest coherence value. To do this, you can use the CoherenceModel(). I did not do this because the runtime to build each LDA model is long."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}