{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3: Textual Data Analysis\n",
    "##### Jade Watson: 20052115"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.idea', 'CISC_CMPE351_W22_E3.pdf', 'corpus.pkl', 'dictionary.gensim', 'Exercise3.ipynb', 'JadeWatsonExercise3.ipynb', 'model1.gensim', 'model1.gensim.expElogbeta.npy', 'model1.gensim.id2word', 'model1.gensim.state', 'model10.gensim', 'model10.gensim.expElogbeta.npy', 'model10.gensim.id2word', 'model10.gensim.state', 'model5.gensim', 'model5.gensim.expElogbeta.npy', 'model5.gensim.id2word', 'model5.gensim.state', 'Questions.csv', 'Questions.csv.zip', 'run_word2vec.ipynb', 'w2v_model.txt']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "datasetPath = r\"C:/Users/Jade Watson/Documents/CMPE 351/Exercise3//\"\n",
    "print(os.listdir(datasetPath))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "         Id  OwnerUserId          CreationDate  Score  \\\n0  43430325    7872366.0  2017-04-15T19:25:31Z      3   \n1  44906264    2659466.0  2017-07-04T12:39:48Z      0   \n2  38765996    5392289.0  2016-08-04T11:21:53Z      2   \n3  42078862    6452857.0  2017-02-06T23:01:53Z      3   \n4  23961089    2743953.0  2014-05-30T18:23:14Z      0   \n5  11252407     720300.0  2012-06-28T20:29:36Z      4   \n6  21534365    1515626.0  2014-02-03T18:03:41Z      0   \n7  13376203    1522730.0  2012-11-14T09:35:33Z      1   \n8  45958533    7100507.0  2017-08-30T10:52:54Z      0   \n9  33068008    5212690.0  2015-10-11T17:33:40Z      0   \n\n                                               Title  \\\n0  How do I split a txt file by html tags or rege...   \n1  Add speaker notes to beamer presentations usin...   \n2  Fit Weibull to distribution with genextreme an...   \n3  R: How to combine rows of a data frame with th...   \n4  Flat-File Authentication, login form does not ...   \n5           XPath to extract text after br tags in R   \n6  Recoding variable into an ordered factor varia...   \n7  Employ environments to handle package-data in ...   \n8  Plot individual-level glm-fit on aggregate-lev...   \n9  how to delete the first n characters of rownam...   \n\n                                                Body  \n0  <p>I have the output of a LexisNexis batch dow...  \n1  <p>I want to create a beamer pdf presentation ...  \n2  <p>Using SciPy, I am trying to reproduce the w...  \n3  <p>Example data frame</p>\\n\\n<pre><code>date  ...  \n4  <p>I am evaluating <a href=\"http://rstudio.git...  \n5  <p>How to extract text after the <code>br</cod...  \n6  <p>I am following the documentation from the <...  \n7  <p>I recently wrote a R extension. The functio...  \n8  <p>In R I am fitting a GLM (logit) on individu...  \n9  <p>I have a data-frame which looks like this:<...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>OwnerUserId</th>\n      <th>CreationDate</th>\n      <th>Score</th>\n      <th>Title</th>\n      <th>Body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>43430325</td>\n      <td>7872366.0</td>\n      <td>2017-04-15T19:25:31Z</td>\n      <td>3</td>\n      <td>How do I split a txt file by html tags or rege...</td>\n      <td>&lt;p&gt;I have the output of a LexisNexis batch dow...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>44906264</td>\n      <td>2659466.0</td>\n      <td>2017-07-04T12:39:48Z</td>\n      <td>0</td>\n      <td>Add speaker notes to beamer presentations usin...</td>\n      <td>&lt;p&gt;I want to create a beamer pdf presentation ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38765996</td>\n      <td>5392289.0</td>\n      <td>2016-08-04T11:21:53Z</td>\n      <td>2</td>\n      <td>Fit Weibull to distribution with genextreme an...</td>\n      <td>&lt;p&gt;Using SciPy, I am trying to reproduce the w...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>42078862</td>\n      <td>6452857.0</td>\n      <td>2017-02-06T23:01:53Z</td>\n      <td>3</td>\n      <td>R: How to combine rows of a data frame with th...</td>\n      <td>&lt;p&gt;Example data frame&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;date  ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23961089</td>\n      <td>2743953.0</td>\n      <td>2014-05-30T18:23:14Z</td>\n      <td>0</td>\n      <td>Flat-File Authentication, login form does not ...</td>\n      <td>&lt;p&gt;I am evaluating &lt;a href=\"http://rstudio.git...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>11252407</td>\n      <td>720300.0</td>\n      <td>2012-06-28T20:29:36Z</td>\n      <td>4</td>\n      <td>XPath to extract text after br tags in R</td>\n      <td>&lt;p&gt;How to extract text after the &lt;code&gt;br&lt;/cod...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>21534365</td>\n      <td>1515626.0</td>\n      <td>2014-02-03T18:03:41Z</td>\n      <td>0</td>\n      <td>Recoding variable into an ordered factor varia...</td>\n      <td>&lt;p&gt;I am following the documentation from the &lt;...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13376203</td>\n      <td>1522730.0</td>\n      <td>2012-11-14T09:35:33Z</td>\n      <td>1</td>\n      <td>Employ environments to handle package-data in ...</td>\n      <td>&lt;p&gt;I recently wrote a R extension. The functio...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>45958533</td>\n      <td>7100507.0</td>\n      <td>2017-08-30T10:52:54Z</td>\n      <td>0</td>\n      <td>Plot individual-level glm-fit on aggregate-lev...</td>\n      <td>&lt;p&gt;In R I am fitting a GLM (logit) on individu...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>33068008</td>\n      <td>5212690.0</td>\n      <td>2015-10-11T17:33:40Z</td>\n      <td>0</td>\n      <td>how to delete the first n characters of rownam...</td>\n      <td>&lt;p&gt;I have a data-frame which looks like this:&lt;...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Style Dataframe\n",
    "df = pd.read_csv(datasetPath + \"Questions.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cleaning\n",
    "##### Lemmatizing and removing the stopwords and non-alphabetic characters for each line of dialogue."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jade\n",
      "[nltk_data]     Watson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jade Watson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "C:\\Users\\Jade Watson\\AppData\\Local\\Temp\\ipykernel_5684\\2985933192.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Title'] =df['Title'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/plain": "         Id  OwnerUserId          CreationDate  Score  \\\n0  43430325    7872366.0  2017-04-15T19:25:31Z      3   \n1  44906264    2659466.0  2017-07-04T12:39:48Z      0   \n2  38765996    5392289.0  2016-08-04T11:21:53Z      2   \n3  42078862    6452857.0  2017-02-06T23:01:53Z      3   \n4  23961089    2743953.0  2014-05-30T18:23:14Z      0   \n\n                                               Title  \\\n0  How do I split a txt file by html tags or rege...   \n1  Add speaker notes to beamer presentations usin...   \n2  Fit Weibull to distribution with genextreme an...   \n3  R  How to combine rows of a data frame with th...   \n4  Flat File Authentication  login form does not ...   \n\n                                                Body  \\\n0  <p>I have the output of a LexisNexis batch dow...   \n1  <p>I want to create a beamer pdf presentation ...   \n2  <p>Using SciPy, I am trying to reproduce the w...   \n3  <p>Example data frame</p>\\n\\n<pre><code>date  ...   \n4  <p>I am evaluating <a href=\"http://rstudio.git...   \n\n                                          clean_text  \n0  split file html tags regex order save separate...  \n1  speaker notes beamer presentations using rmark...  \n2            weibull distribution genextreme weibull  \n3          combine rows data frame take newest value  \n4  flat file authentication login form shown shin...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>OwnerUserId</th>\n      <th>CreationDate</th>\n      <th>Score</th>\n      <th>Title</th>\n      <th>Body</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>43430325</td>\n      <td>7872366.0</td>\n      <td>2017-04-15T19:25:31Z</td>\n      <td>3</td>\n      <td>How do I split a txt file by html tags or rege...</td>\n      <td>&lt;p&gt;I have the output of a LexisNexis batch dow...</td>\n      <td>split file html tags regex order save separate...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>44906264</td>\n      <td>2659466.0</td>\n      <td>2017-07-04T12:39:48Z</td>\n      <td>0</td>\n      <td>Add speaker notes to beamer presentations usin...</td>\n      <td>&lt;p&gt;I want to create a beamer pdf presentation ...</td>\n      <td>speaker notes beamer presentations using rmark...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38765996</td>\n      <td>5392289.0</td>\n      <td>2016-08-04T11:21:53Z</td>\n      <td>2</td>\n      <td>Fit Weibull to distribution with genextreme an...</td>\n      <td>&lt;p&gt;Using SciPy, I am trying to reproduce the w...</td>\n      <td>weibull distribution genextreme weibull</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>42078862</td>\n      <td>6452857.0</td>\n      <td>2017-02-06T23:01:53Z</td>\n      <td>3</td>\n      <td>R  How to combine rows of a data frame with th...</td>\n      <td>&lt;p&gt;Example data frame&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;date  ...</td>\n      <td>combine rows data frame take newest value</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23961089</td>\n      <td>2743953.0</td>\n      <td>2014-05-30T18:23:14Z</td>\n      <td>0</td>\n      <td>Flat File Authentication  login form does not ...</td>\n      <td>&lt;p&gt;I am evaluating &lt;a href=\"http://rstudio.git...</td>\n      <td>flat file authentication login form shown shin...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['Title'] =df['Title'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "stopwords_list = stopwords.words('english')\n",
    "punctuations = list(set(string.punctuation))\n",
    "\n",
    "def clean_text_initial(text):\n",
    "    text = ' '.join([x.lower() for x in word_tokenize(text) if x.lower() not in stopwords_list and len(x)>1])\n",
    "    text = ' '.join([x.lower() for x in word_tokenize(text) if x.lower() not in punctuations and len(x)>3])\n",
    "    return text.strip()\n",
    "\n",
    "df[\"clean_text\"]=df.Title.apply(lambda text:clean_text_initial(str(text)))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I decided to lemmanize the titles instaed of stemming them directly because it stems the word but ensures that it does not loose its meaning. Lemmatization has a pre-defined dictionary that stores the context of words and checks the word in the dictionary."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93momw-1.4\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/omw-1.4\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Jade Watson/nltk_data'\n    - 'C:\\\\Users\\\\Jade Watson\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jade Watson\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jade Watson\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jade Watson\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 84\u001B[0m     root \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mzip_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93momw-1.4\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/omw-1.4.zip/omw-1.4/\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Jade Watson/nltk_data'\n    - 'C:\\\\Users\\\\Jade Watson\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jade Watson\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jade Watson\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jade Watson\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstem\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WordNetLemmatizer\n\u001B[0;32m      4\u001B[0m lemmatizer \u001B[38;5;241m=\u001B[39m WordNetLemmatizer()\n\u001B[1;32m----> 6\u001B[0m cleaned_text_list\u001B[38;5;241m=\u001B[39m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclean_text\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclean_text\u001B[49m\u001B[43m:\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlemmatizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlemmatize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenized_text\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtokenized_text\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclean_text\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m gensim_dict\u001B[38;5;241m=\u001B[39mDictionary(cleaned_text_list)\n\u001B[0;32m     10\u001B[0m doc_term_matrix \u001B[38;5;241m=\u001B[39m [gensim_dict\u001B[38;5;241m.\u001B[39mdoc2bow(text) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m cleaned_text_list]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4357\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4247\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4248\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4249\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4252\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4253\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m FrameOrSeriesUnion:\n\u001B[0;32m   4254\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4255\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4256\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4355\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4356\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4357\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1043\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m   1040\u001B[0m     \u001B[38;5;66;03m# if we are a string, try to dispatch\u001B[39;00m\n\u001B[0;32m   1041\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m-> 1043\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1098\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1092\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m   1093\u001B[0m         \u001B[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001B[39;00m\n\u001B[0;32m   1094\u001B[0m         \u001B[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001B[39;00m\n\u001B[0;32m   1095\u001B[0m         \u001B[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001B[39;00m\n\u001B[0;32m   1096\u001B[0m         \u001B[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001B[39;00m\n\u001B[0;32m   1097\u001B[0m         \u001B[38;5;66;03m# \"Callable[[Any], Any]\"\u001B[39;00m\n\u001B[1;32m-> 1098\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1099\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1100\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1101\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1102\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1105\u001B[0m     \u001B[38;5;66;03m# GH 25959 use pd.array instead of tolist\u001B[39;00m\n\u001B[0;32m   1106\u001B[0m     \u001B[38;5;66;03m# so extension arrays can be used\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(pd_array(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2859\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(clean_text)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstem\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WordNetLemmatizer\n\u001B[0;32m      4\u001B[0m lemmatizer \u001B[38;5;241m=\u001B[39m WordNetLemmatizer()\n\u001B[1;32m----> 6\u001B[0m cleaned_text_list\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39mclean_text\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m clean_text:[lemmatizer\u001B[38;5;241m.\u001B[39mlemmatize(tokenized_text) \u001B[38;5;28;01mfor\u001B[39;00m tokenized_text \u001B[38;5;129;01min\u001B[39;00m word_tokenize(clean_text)])\n\u001B[0;32m      8\u001B[0m gensim_dict\u001B[38;5;241m=\u001B[39mDictionary(cleaned_text_list)\n\u001B[0;32m     10\u001B[0m doc_term_matrix \u001B[38;5;241m=\u001B[39m [gensim_dict\u001B[38;5;241m.\u001B[39mdoc2bow(text) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m cleaned_text_list]\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstem\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WordNetLemmatizer\n\u001B[0;32m      4\u001B[0m lemmatizer \u001B[38;5;241m=\u001B[39m WordNetLemmatizer()\n\u001B[1;32m----> 6\u001B[0m cleaned_text_list\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39mclean_text\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m clean_text:[\u001B[43mlemmatizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlemmatize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenized_text\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m tokenized_text \u001B[38;5;129;01min\u001B[39;00m word_tokenize(clean_text)])\n\u001B[0;32m      8\u001B[0m gensim_dict\u001B[38;5;241m=\u001B[39mDictionary(cleaned_text_list)\n\u001B[0;32m     10\u001B[0m doc_term_matrix \u001B[38;5;241m=\u001B[39m [gensim_dict\u001B[38;5;241m.\u001B[39mdoc2bow(text) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m cleaned_text_list]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001B[0m, in \u001B[0;36mWordNetLemmatizer.lemmatize\u001B[1;34m(self, word, pos)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlemmatize\u001B[39m(\u001B[38;5;28mself\u001B[39m, word: \u001B[38;5;28mstr\u001B[39m, pos: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m     lemmas \u001B[38;5;241m=\u001B[39m \u001B[43mwn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_morphy\u001B[49m(word, pos)\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmin\u001B[39m(lemmas, key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m lemmas \u001B[38;5;28;01melse\u001B[39;00m word\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__getattr__\u001B[1;34m(self, attr)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLazyCorpusLoader object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 121\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# __class__ to something new:\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, attr)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:89\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     86\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# Load the corpus.\u001B[39;00m\n\u001B[1;32m---> 89\u001B[0m corpus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__reader_cls(root, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__kwargs)\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001B[39;00m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001B[39;00m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;66;03m# match that of the corpus.\u001B[39;00m\n\u001B[0;32m     95\u001B[0m args, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__kwargs\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1176\u001B[0m, in \u001B[0;36mWordNetCorpusReader.__init__\u001B[1;34m(self, root, omw_reader)\u001B[0m\n\u001B[0;32m   1172\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1173\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe multilingual functions are not available with this Wordnet version\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1174\u001B[0m     )\n\u001B[0;32m   1175\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1176\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprovenances \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43momw_prov\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1178\u001B[0m \u001B[38;5;66;03m# A cache to store the wordnet data of multiple languages\u001B[39;00m\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lang_data \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mlist\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1285\u001B[0m, in \u001B[0;36mWordNetCorpusReader.omw_prov\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1283\u001B[0m provdict \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m   1284\u001B[0m provdict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meng\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1285\u001B[0m fileids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_omw_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfileids\u001B[49m()\n\u001B[0;32m   1286\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fileid \u001B[38;5;129;01min\u001B[39;00m fileids:\n\u001B[0;32m   1287\u001B[0m     prov, langfile \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplit(fileid)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__getattr__\u001B[1;34m(self, attr)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLazyCorpusLoader object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__bases__\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 121\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# __class__ to something new:\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, attr)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     84\u001B[0m             root \u001B[38;5;241m=\u001B[39m nltk\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubdir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzip_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     85\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m:\n\u001B[1;32m---> 86\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# Load the corpus.\u001B[39;00m\n\u001B[0;32m     89\u001B[0m corpus \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__reader_cls(root, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__kwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001B[0m, in \u001B[0;36mLazyCorpusLoader.__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 81\u001B[0m         root \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubdir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     83\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    581\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[0;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93momw-1.4\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/omw-1.4\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Jade Watson/nltk_data'\n    - 'C:\\\\Users\\\\Jade Watson\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jade Watson\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jade Watson\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Jade Watson\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "cleaned_text_list=df.clean_text.apply(lambda clean_text:[lemmatizer.lemmatize(tokenized_text) for tokenized_text in word_tokenize(clean_text)])\n",
    "\n",
    "gensim_dict=Dictionary(cleaned_text_list)\n",
    "\n",
    "doc_term_matrix = [gensim_dict.doc2bow(text) for text in cleaned_text_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df.clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bigrams\n",
    "##### Used to detect common phrases from a list of sentances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# make list a series\n",
    "clean_text_series = pd.Series(cleaned_text_list)\n",
    "\n",
    "sent = [row.split() for row in df.clean_text]\n",
    "# create the relevant phrases from the list of sentances\n",
    "phrases = Phrases(sent,min_count=30,progress_per=10000)\n",
    "# use phraser() to cut down memory consumption of phrases() by discarding model state not needed for bigram detection task\n",
    "bigram = Phraser(phrases)\n",
    "# transform the corpus based on the bigrams detected:\n",
    "sentences = bigram[sent]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Most Frequent Words\n",
    "##### Used to check the effectiveness of the lemmatization and addition of bigrams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### i) Word2Vec(): set up parameters of the model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "# count the number of cores in a computer\n",
    "cores = multiprocessing.cpu_count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=25,window=2,size=300,sample=6e-5,alpha=0.03,min_alpha=0.0007,negative=20,workers=cores-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### ii) Build Vocabulary Table: build vocab from a sequence of sentences to initialize the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build the vocabulary table\n",
    "t = time()\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "print('Time to build vocab table: {} mins'.format(round((time()-t)/60,2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### iii) Train the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = time()\n",
    "# where total_examples is the count of sentences\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make model more memory-efficient\n",
    "w2v_model.init_sims(replace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learned_words = list(w2v_model.wv.vocab)\n",
    "print(learned_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save model\n",
    "w2v_model.wv.save_word2vec_format('w2v_model.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploring the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=['plots'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2v_model.wv.similarity('plot','answer')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Topic Modelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(text) for text in sentences]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', ldamodel.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 10 topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model10.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', ldamodel.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 5 Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "According to our notes, calculating the perplexity is how one chooses the number of topics. To calculate perplexity the inverse log-likelihood of the dictionary above is taken. We wish to have models with lower perplexity because it suggests less uncertainties about the unobserved document. I tested this theory by varying the number of topics. I first tested with 5 topics. It is evident that topic 1 contains words like function, using, loop, and error. Topics 1-4 are printed above as well. Next I tested with 10 topics. Topic 6 contains words like data, table, extract, and remove. The weights in front of each words reflects how important the keyword is to the topic. So the higher the weight the more it contributes to the topic. In the case above, the preferred number of topics is 5. Another approach to finding the optimal number of topics is to build many LDA models with different topic number and pick one that gives the highest coherence value. To do this, you can use the CoherenceModel(). I did not do this because the runtime to build each LDA model is long."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}